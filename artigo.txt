BPE-Dropout: regularização simples e eficaz da regularização de Ivan Protilkov [] [1] [,] [2] Dmitrii Emelianenko [] [1] [,] [3] Elena Voita [4] [,] [5] Universidade da Universidade, Rússia 2 Moscou, Rousics and Technology, Russia 3 Amsterdã, Holanda {IV-Provilkov, Dimdi-Y, Lena-Voita}} @yandex-team.ru A segmentação da subglema abstrata é amplamente usada para resolver o problema de vocabulário aberto na tradução da máquina. A abordagem dominante para a segmentação da subglema é a codificação de pares de bytes (BPE), que mantém as palavras mais frequentes intactas ao dividir as raras em vários tokens. Embora várias segmentações sejam possíveis, mesmo com o mesmo vocabulário, o BPE divide palavras em seqüências únicas; Isso pode impedir que um modelo aprenda melhor a composicionalidade das palavras e seja robusta para erros de segmentação. Até agora, a única maneira de superar essa imperfeição do BPE, sua natureza determinística, era criar outro algoritmo de segmentação de subgelas (Kudo, 2018). Por outro lado, mostramos que o próprio BPE incorpora a capacidade de produzir múltiplas segmentações da mesma palavra. Introduzimos o método de regularização simples e eficaz da subglema BPE-Dropout com base e compatível com o BPE convencional. Ele corrompe estocasticamente o procedimento de segmentação do BPE, o que leva à produção de múltiplas segmentações dentro da mesma estrutura de BPE fixa. O uso do BPE-Dropout durante o treinamento e o BPE padrão durante a inferência melhora a qualidade da tradução de até 2,3 bleu em comparação com o BPE e até 0,9 bleu em comparação com a regularização anterior da subglema. 1 Introdução O uso da segmentação de subgletas tornou-se padrão de fato na tradução da máquina neural (Bojar et al., 2018; Barroult et al., 2019). A codificação de pares de bytes (BPE) (Sennrich et al., 2016) é a abordagem dominante para a segmentação da subglema. Ele mantém as palavras comuns intactas ao dividir as raras e desconhecidas em uma sequência de unidades de subglema. Isso potencialmente permite que um modelo faça uma contribuição igual. Uso da morfologia, composição de palavras e transliteração. O BPE lida efetivamente com um problema OpenVocabulary e é amplamente utilizado devido à sua simplicidade. Há, no entanto, uma desvantagem do BPE em sua natureza determinística: divide as palavras em sequências de subgletas exclusivas, o que significa que, para cada palavra, um modelo observa apenas uma segmentação. Assim, é provável que um modelo não atinja todo o seu potencial na exploração da morfologia, aprendendo a composicionalidade das palavras e sendo robusta para erros de segmentação. Além disso, como mostraremos mais, subpainhas nas quais palavras raras são segmentadas acabam pouco compreendidas. Uma maneira natural de lidar com esse problema é permitir vários candidatos a segmentação. Isso foi proposto inicialmente por Kudo (2018) como uma regularização da subglema Um método de regularização, que é implementado como uma amostragem de dados on-the-fly e não é específica para a arquitetura NMT. Como o BPE padrão produz segmentação única, para perceber essa regularização, o autor teve que propor uma nova segmentação de subgletas, diferente do BPE. No entanto, a abordagem introduzida é bastante complicada: requer treinamento de um modelo de idioma de segmentação separado, usando algoritmos EM e Viterbi e proíbe usando o BPE convencional. Por outro lado, mostramos que o próprio BPE incorpora a capacidade de produzir múltiplas segmentações da mesma palavra. O BPE constrói um vocabulário de subpainhas e uma tabela de mesclagem, que especifica quais subpainhas devem ser mescladas em uma subglema maior, bem como a prioridade das mescladas. Durante a segmentação, as palavras são divididas primeiro em sequências de caracteres, depois as operações de mesclagem aprendidas são aplicadas para mesclar os caracteres em símbolos maiores e conhecidos, até que nenhuma mescla possa ser feita (Figura 1 (a)). Introduzimos o BPE-Dropout um método de regularização da subglema baseado e compatível com o BPE convencional. Ele usa um vocabulário e um processo de 1882 da 58ª Reunião Anual da Associação de Linguística Computacional, páginas 18821892 de 5 a 10 de julho de 2020. Os hífens indicam possíveis mescle (mesclar que estão presentes na tabela de mesclagem); As mescladas realizadas em cada iteração são mostradas em verde, jogadas em vermelho. Tabela de mesclagem construída pelo BPE, mas em cada etapa de mesclagem, algumas mesclares são descartadas aleatoriamente. Isso resulta em diferentes segmentações para a mesma palavra (Figura 1 (b)). Nosso método não requer treinamento de segmentação além do BPE e usa o BPE padrão no tempo de teste, portanto, é simples. O BPE-Dropout é superior em comparação com o BPE e o Kudo (2018) em uma ampla gama de tarefas de tradução, portanto, é eficaz. Nossas principais contribuições são as seguintes: Apresentamos o BPE-Dropout um método simples e eficaz de regularização da subglema;  Mostramos que nosso método supera o BPE e a regularização anterior da subglema em uma ampla gama de tarefas de tradução;  Analisamos como o treinamento com o BPE-Dropout afeta um modelo e mostramos que ele leva a uma qualidade de aposta de incorporações de token aprendidas e a um modelo ser mais robusto para a entrada barulhenta. 2 Antecedentes Nesta seção, descrevemos brevemente o BPE e o conceito de regularização da subglema. Assumimos que nossa tarefa é a tradução da máquina, onde um modelo precisa prever a frase de destino y, dada a frase de origem X, mas os métodos que descrevemos não são específicos de tarefas. 2.1 Codificação de pares de bytes (BPE) para definir um procedimento de segmentação, o BPE (Sennrich et al., 2016) constrói um vocabulário simbólico e uma tabela de mesclagem. O vocabulário do token é inicializado com o vocabulário do personagem e a tabela de mesclagem é inicializada com uma tabela vazia. Primeiro, cada palavra é representada como uma sequência de tokens, além de um símbolo especial de fim de palavra. Em seguida, o método conta iterativamente todos os pares de tokens e mescla o par mais frequente em um novo token. Este token é onde X e Y são candidatos a segmentação amostrados para as sentenças x e y, respectivamente, p (x | x) e p (y | y) são as distribuições de probabilidade da qual os candidatos são amostrados e é o conjunto de parâmetros do modelo. Na prática, em cada etapa de treinamento, apenas um candidato a segmentação é amostrado. Como a segmentação padrão do BPE é determinística, para realizar essa regularização Kudo (2018) propôs uma nova segmentação de subgletas. A abordagem introduzida requer o treinamento de um modelo de idioma de segmentação separado para prever a probabilidade de cada subglema, o algoritmo EM para otimizar o vocabulário e o algoritmo Viterbi para fazer amostras de segmentações. A regularização da subglema foi demonstrada para obter melhorias significativas em relação ao método usando uma única sequência de subgletas. No entanto, o método proposto é bastante complicado e proíbe usando adicionado ao vocabulário, e a operação de mesclagem é adicionada à tabela de mesclagem. Isso é feito até que o tamanho do vocabulário desejado seja atingido. A tabela de mesclagem resultante especifica quais subpainhas devem ser mescladas em uma subglema maior, bem como a prioridade das mescladas. Dessa maneira, ele define o procedimento de segmentação. Primeiro, uma palavra é dividida em caracteres distintos, além do final do símbolo da palavra. Em seguida, o par de tokens adjacentes, que tem a maior prioridade, é mesclada. Isso é feito iterativamente até que nenhuma fusão da tabela esteja disponível (Figura 1 (a)). 2.2 A regularização da subglema da subglema (Kudo, 2018) é um algoritmo de treinamento que integra vários candidatos a segmentação. Em vez de maximizar a probabilidade de log, esse algoritmo maximiza a probabilidade de log marginalizada em diferentes candidatos a segmentação. Formalmente, l = e log p (y | x,), (1) e (x, y) d xypp ((xy || xy)) log p (y | x,), (1) 1883 BPE convencional. Isso pode impedir que os profissionais usem a regularização da subglema. 3 Nossa abordagem: BPE-Dropout, mostramos que, para realizar a regularização da subglema, não é necessário rejeitar o BPE, pois vários candidatos a segmentação podem ser gerados dentro da estrutura do BPE. Introduzimos o BPE-Dropout um método que explora a capacidade inata do BPE para ser estocástica. Ele altera o procedimento de segmentação, mantendo a tabela de mesclagem BPE original. Durante a segmentação, em cada etapa de mesclagem, algumas mescladas são descartadas aleatoriamente com a probabilidade p. Este procedimento é descrito no algoritmo 1. - Algoritmo 1: BPE DROPOUT CTRANHA DE DISPAÇÃO DA PALAVRA DE INPUT; Faça mescla todas as mesclas possíveis [1] dos tokens da divisão atual; Para a fusão das mescladas, a única diferença de BPE - [ /] remova a fusão das mescladas com a probabilidade p; final Se fusão não estiver vazia, a mesclagem, selecione a mesclagem com a maior prioridade das mescladas; aplique a fusão à divisão atual; terminar enquanto mescla não está vazio; Retorno atual S P Lit; Se P estiver definido como 0, a segmentação é equivalente ao BPE padrão; Se P estiver definido como 1, a segmentação divide palavras em caracteres distintos. Os valores entre 0 e 1 podem ser usados ​​para controlar a granularidade da segmentação. Usamos p> 0 (geralmente p = 0. 1) no tempo de treinamento para expor um modelo a diferentes segmentações e p = 0 durante a inferência, o que significa que, no tempo de inferência, usamos o BPE original. Discutimos a escolha do valor de P na Seção 5. Quando algumas mescladas são proibidas aleatoriamente durante a segmentação, as palavras acabam segmentadas em diferentes subbordas; Veja, por exemplo, Figura 1 (b). Nossa hipótese é que a exposição de um modelo a 1 no caso de múltiplas ocorrências da mesma mesclagem em uma palavra (por exemplo, M-E-R-G-E-R tem duas ocorrências da mesclagem (e, r)), decidimos de forma independente para cada ocorrência que o abandone ou não. As segmentações podem resultar em uma melhor compreensão de todas as palavras, bem como em suas unidades de subglema; Vamos verificar isso na Seção 6. 4 Configuração experimental 4.1 Linhas de base de base Nossas linhas de base são o BPE padrão e a regularização da sub -palavra de Kudo (2018). A regularização da subglema por Kudo (2018) possui hiperparameters de amostragem de segmentação L e. l Especifica quantas melhores segmentações para cada palavra são produzidas antes de provar um deles, controla a suavidade da distribuição de amostragem. No artigo original (l =, = 0. 2 /0. 5) e (L = 64, = 0. 1) demonstrou ter um desempenho melhor em diferentes conjuntos de dados. Como no geral, eles mostram resultados comparáveis, em todas as experiências que usamos (L = 64, = 0. 1). 4.2 Vocabulários Existem duas maneiras de criar vocabulário para modelos treinados com o BPE-Dropout: (1) Pegue o vocabulário construído pelo BPE; Em seguida, o texto segmentado com BPE-Dropout conterá um pequeno número de tokens desconhecidos (UNKs) [2]; (2) Adicione ao vocabulário BPE todos os tokens que podem aparecer ao segmentar com o BPE-Dropout. Nos experimentos preliminares, não observamos nenhuma diferença de qualidade; Portanto, qualquer um dos métodos pode ser usado. Escolhemos a primeira opção para permanecer na mesma configuração que o BPE padrão. Além disso, um modelo exposto a algumas unidades no treinamento pode ser mais confiável para aplicações práticas, onde tokens desconhecidos podem estar presentes. 4.3 Conjuntos de dados e pré -processamento Nós conduzimos nossos experimentos em uma ampla gama de conjuntos de dados com diferentes tamanhos e idiomas de corpora; As informações sobre os conjuntos de dados estão resumidas na Tabela 1. Esses conjuntos de dados são usados ​​nos principais experimentos (Seção 5.1) e foram escolhidos para corresponder aos usados ​​no trabalho anterior (Kudo, 2018). Nas experiências adicionais (Seções 5.2-5.5), também usamos subconjuntos aleatórios dos dados do WMT14 Inglês-Francês; Nesse caso, especificamos o tamanho do conjunto de dados para cada experimento. Antes da segmentação, pré -processamos todos os 2, por exemplo, para a parte inglesa do IWSLT15 EnvI Corpora, essas unidades compõem 0,00585 e 0,00085 de todos os tokens para vocabulários de 32k e 4K, respectivamente. 1884 Número de frases Tamanho Voc Tamanho em lote O valor de P (trem / dev / teste) no BPE-Dropout IWSLT15 EN VI 133K / 1553 /1268 4K 4K 0,1 / 0,1 EN ZH 209K / 887 /1261 4K 4K 0.1 / 0,10 / 888 /1205 4K 4K 0,1 / 0,1 WMT14 ET DE 4,5M / 3000 /3003 32K 32K 0,1 / 0.1 ASPEC EN JA 2M / 1700 /1812 16K 32K 0,1 / 0.6 Tabela 1: Visão geral dos dados e hiperparamettes dependentes de dados de dados; Os valores de P são mostrados em pares: linguagem de origem / linguagem de destino. (Explicamos a escolha do valor de P para BPE-Dropout na Seção 5.3.) conjuntos de dados com o kit de ferramentas padrão de Moses. [3] No entanto, chinês e japonês não têm limites explícitos de palavras, e o tokenizador de Moisés não segenta frases em palavras; Para esses idiomas, as segmentações de subgletas são treinadas quase a partir de frases brutas não segmentadas. Baseando -se em um estudo recente de como a escolha do tamanho do vocabulário influencia a qualidade da tradução (Ding et al., 2019), escolhemos o tamanho do vocabulário, dependendo do tamanho do conjunto de dados (Tabela 1). No treinamento, os pares de tradução foram juntos por comprimento aproximado da sequência. Para os principais experimentos, os valores do tamanho do lote que usamos são apresentados na Tabela 1 (o tamanho do lote é o número de tokens de origem). Nas experiências nas seções 5.2, 5.3 e 5.4, para conjuntos de dados não maiores que 500 mil pares de frases, usamos o tamanho do vocabulário e o tamanho do lote de 4K e 32k para o restante. [4] No texto principal, treinamos todos os modelos em dados mais baixos. No apêndice, fornecemos experimentos adicionais com o estojo original e o estojo de caixa. 4.4 Modelo e otimizador O sistema NMT usado em nossos experimentos é a base de transformadores (Vaswani et al., 2017). Mais precisamente, o número de camadas é n = 6 com h = 8 camadas de atenção paralelas ou cabeças. A dimensionalidade da entrada e saída é D Modelo = 512, e a camada interna das redes de feed-forward possui dimensionalidade d ff = 2048. Utilizamos procedimento de regularização e otimização, conforme descrito em Vaswani et al. (2017). 3 [https://github.com/moses-smt/](https://github.com/moses-smt/mosesdecoder) [mosesdecoder](https://github.com/moses-smt/mosesdecoder) 4 Large batch size can be reached by using several of GPUs or by accumulating the gradients for several batches and then making an update. 4.5 Tempo de treinamento Treinamos modelos até a convergência. Para todas as experiências, fornecemos um número de lotes de treinamento no apêndice (Tabelas 6 e 7). 4.6 Inferência para produzir traduções, para todos os modelos, usamos a pesquisa de feixe com o feixe de 4 e a normalização do comprimento de 0,6. Além dos principais resultados, Kudo (2018) também relata as pontuações usando a decodificação N -Best. Para traduzir uma frase, essa estratégia produz várias segmentações de uma frase de origem, gera uma tradução para cada uma delas e resgate as traduções obtidas. Embora este possa ser um trabalho futuro interessante para investigar diferentes estratégias de amostragem e resgate, no presente estudo, usamos uma melhor decodificação para se encaixar no paradigma de decodificação padrão. 4.7 Avaliação para avaliação, calculamos a média de 5 pontos de verificação mais recentes e usamos Bleu (Papineni et al., 2002) calculados via Sacrebleu [5] (Post, 2018). Para chinês, adicionamos opção -Tok Zh a Sacrebleu. Para japonês, usamos o bleu baseado em personagens. 5 Experimentos 5.1 Resultados principais Os resultados são fornecidos na Tabela 2. Para todos os conjuntos de dados, o BPE-Dropout melhora significativamente em relação ao BPE padrão: mais de 1,5 bleu para en-VI, Vi-en, en-ZH, ZH-en, ar-en, de-en e 0,5-1.4 5 nossa assinatura sacrebleu é: bleu+ case.lc+ lang. [src-lang]-[dst-lang]+numrefs.1+smooth.exp+tok.13a+versão.1.3.6 1885 bpe kudo (2018) bpe-arpout iwst15 en-V 31,78 32.43 33.27 Vi-sen 30.83 32.36 32.9 32.9 32.9. 19.72 21.10 21.45 IWSLT17 En-Fr 39.37 39.45 40.02 Fr-En 38.18 38.88 39.39 En-Ar 13.89 14.43 15.05 Ar-En 31.90 32.80 33.72 WMT14 En-De 27.41 27.82 28.01 De-En 32.69 33.65 34.19 ASPEC EN-JA 54.51 55.46 55,00 JA-EN 30,77 31,23 31.29 Tabela 2: Pontuações Bleu. O BOLD indica a melhor pontuação e todas as pontuações cuja diferença do melhor não é estaticamente significativa (com o valor p de 0,05). (A significância estatística é calculada via bootstrapping (Koehn, 2004).) Bleu para o resto. As melhorias são especialmente proeminentes para conjuntos de dados menores; Discutiremos isso mais adiante na Seção 5.4. Comparado a Kudo (2018), entre os 12 conjuntos de dados que usamos o BPE-Dropout é benéfico para 8 conjuntos de dados com melhorias de até 0,92 bleu, não é significativamente diferente para 3 conjuntos de dados e um desempenho inferior apenas em EN-JA. Enquanto Kudo (2018) usa outra segmentação, nosso método opera dentro da estrutura do BPE e altera apenas a maneira como um modelo é treinado. Assim, o menor desempenho do BPE-Dropout no EN-JA e apenas diferenças pequenas ou insignificantes para JA-en, en-Zh e Zhen sugerem que japoneses e chineses podem se beneficiar de uma segmentação específica do idioma. Observe também que Kudo (2018) relata maiores melhorias em relação ao BPE ao usar seu método do que mostramos na Tabela 2. Isso pode ser explicado pelo fato de Kudo (2018) usar grande tamanho de vocabulário (16k, 32k), que foi mostrado contraproducente para pequenos conjuntos de dados (Sennrich e Zhang, 2019; Ding et al., 2019). Embora esse possa não ser o problema dos modelos treinados com a regularização do subpainhas (consulte a Seção 5.4), isso causa queda drástica no desempenho das linhas de base. BPE BPE-Dropout somente DST Src apenas 250k 26,94 27,98 27,71 28,40 500K 29.28 30.12 29.40 29.89 1M 30.53 31,09 30.62 31,23 33.38 33.89 33.46 33.85 165 16m 40.62 33.33333 33.89 33.46 33.85 165 162. Treinado com o desgaste do BPE em um lado único de um par de tradução ou de ambos os lados. Modelos treinados em subconjuntos aleatórios do conjunto de dados WMT14 EN-FR. Bold indica a melhor pontuação e todas as pontuações cuja diferença do melhor não é estatística significativa (com valor p de 0,05). 5.2 Lateral único versus regularização completa Nesta seção, investigamos se o desgaste do BPE deve ser usado apenas em um lado de um par de tradução ou para idiomas de origem e de destino. Selecionamos subconjuntos aleatórios de tamanhos diferentes dos dados WMT14 EN-FR para entender como os resultados são afetados pela quantidade de dados. Mostramos isso: para conjuntos de dados pequenos e médios, a regularização completa tem um desempenho melhor;  Para conjuntos de dados grandes, o BPE-Dropout deve ser usado apenas no lado da fonte. Como a regularização completa tem o melhor desempenho para a maioria dos tamanhos de dados considerados, nas seções subsequentes, usamos o BPE-Dropout nos lados de origem e de destino. 5.2.1 conjuntos de dados pequenos e médios: Use a regularização completa A Tabela 3 indica que o uso do BPE-Dropout no lado da fonte é mais benéfico do que no lado do destino; Para os conjuntos de dados não menores que os pares de frases de 0,5M, o BPE-Dropout pode ser usado apenas o lado da fonte. Podemos especular que é mais importante para o modelo entender uma frase de origem do que ser exposto a maneiras diferentes de gerar a mesma frase de destino. 5.2.2 conjuntos de dados grandes: Use apenas para fonte para corpora maior (por exemplo, a partir de instâncias 4M), é melhor usar o BPE-Dropout apenas no lado da fonte (Tabela 3). Curiosamente, o uso do BPE-Dropout para idiomas de origem e de destino prejudica o desempenho de grandes conjuntos de dados. 1886 Figura 2: Pontuações Bleu para os modelos treinados com BPE-Dropout com diferentes valores de p. WMT14 ENFR, pares de 500 mil frases. 5.3 Escolha do valor de P A Figura 2 mostra as pontuações Bleu para os modelos treinados no BPE-Dropout com diferentes valores de p (a probabilidade de uma mesclagem sendo descartada). Os modelos treinados com altos valores de P são incapazes de se traduzir devido a uma grande incompatibilidade entre a segmentação de treinamento (que está próxima do nível de char) e segmentação de inferência (BPE). A melhor qualidade é alcançada com p = 0. 1. Em nossos experimentos, usamos p = 0. 1 para todos os idiomas, exceto em chinês e japonês. Para chinês e japonês, tomamos o valor de p = 0. 6 Para corresponder ao aumento do comprimento das frases segmentadas para outros idiomas. [6] 5.4 CORPORA VÁRIL E TAMANHO DE VOCABULÁRIO Agora, examinaremos mais de perto como a melhoria do uso do BPE-Dropout depende do corpora e do tamanho do vocabulário. Primeiro, vemos que o BPE-Dropout tem um desempenho melhor para todos os tamanhos de conjunto de dados (Figura 3). Em seguida, os modelos treinados com a regularização da subglema são menos sensíveis à escolha do tamanho do vocabulário: as diferenças no desempenho dos modelos com vocabulário 4K e 32K são muito menores do que os modelos treinados com o BPE padrão. Isso torna o BPE-Dropout atraente, pois permite (i) não sintonizar o tamanho do vocabulário para cada conjunto de dados, (ii) escolha o tamanho do vocabulário, dependendo das propriedades do modelo desejado: os modelos com vocabulários menores são benéficos em termos de número de parâmetros, modelos com vocabulários maiores são benéficos em termos de tempo de infância. [7] Finalmente, vemos que o efeito de usar 6 formalmente, para inglês/francês/etc. com BPE-Dropout, p = 0. 1 sentenças tornam -se em média cerca de 1,25 vezes mais em comparação com segmentadas com BPE; Para chinês e japonês, precisamos definir o valor de P a 0. 6 para obter o mesmo aumento. 7 A Tabela 4 mostra que a inferência por modelos com 4K Vocab Figura 3: Bleu Scores. Modelos treinados em subconjuntos aleatórios de WMT14 EN-FR. O BPE-Dropout desaparece quando um tamanho de corpora fica maior. Isso não é surpreendente: o efeito de qualquer regularização é menos em configurações de alto recurso; No entanto, como mostraremos mais adiante na Seção 6.3, quando aplicado à fonte barulhenta, os modelos treinados com o desgaste do BPE mostram melhorias substanciais de até 2 bleu, mesmo em configurações de alto relevo. Observe que, para corpora maior, recomendamos o uso do BPE-Dropout apenas para linguagem de origem (Seção 5.2). 5.5 Tempo de inferência e comprimento das seqüências geradas Como o BPE-Dropout produz mais segmentação de granulação fina, as sentenças segmentadas com o desgaste do BPE são mais longas; A distribuição dos comprimentos da frase é mostrada na Figura 4 (a) (com p = 0. 1, em média cerca de 1,25 vezes mais). Assim, existe um perigo potencial de que os modelos treinados com o desgaste do BPE podem tender a usar mais segmentação de granulação fina em inferência e, portanto, para diminuir a decepção. No entanto, na prática, esse não é o caso: as distribuições de comprimentos das traduções geradas para modelos treinados com BPE e com o desgaste do BPE são próximos (Figura 4 (b)). [8] A Tabela 4 confirma essas observações e mostra que o tempo de inferência dos modelos treinados com o desgaste do BPE não é substancialmente diferente dos treinados com BPE. ULARY é mais de 1,4 vezes mais que os modelos com 32K de vocabulário. 8 Este é o resultado do uso da pesquisa de feixe: enquanto amostras de um modelo reproduzem muito bem o treinamento de dados de dados, a pesquisa de feixe favorece tokens mais frequentes (Ott et al., 2018). Portanto, as traduções de pesquisa de feixe tendem a não usar segmentação de granulação fina menos frequente. 1887 (a) (b) Figura 4: Distribuições de comprimento (em tokens) de (a) a parte francesa do WMT14 EN-FR Conjunto de testes segmentados usando BPE ou BPE-Dropout; e (b) as traduções geradas para o mesmo teste definido por modelos treinados com BPE ou BPE-Dropout. Tabela 4: Tempo de inferência relativa dos modelos treinados com diferentes métodos de segmentação de sub -palavras. Os resultados obtidos por (1) a computação em média mais de 1000 execuções de tempo necessárias para traduzir o conjunto de testes WMT14 EN-FR, (2) dividindo todos os resultados pelo menor dos tempos obtidos. 6 Análise nesta seção, analisamos diferenças qualitativas entre os modelos treinados com o BPE e o BPE-abandono. Descobrimos que, ao usar o BPE, sequências frequentes de caracteres raramente aparecem em um texto segmentado como tokens individuais, em vez de ser uma parte maior; O BPE-Dropout alivia essa questão;  Ao analisar os espaços de incorporação instruídos, mostramos que o uso do BPE-Dropout leva a uma melhor compreensão dos tokens raros;  Como conseqüência do exposto, os modelos treinados com o BPE-Dropout são mais robustos para a entrada incorreta. 6.1 Frequência de substring Aqui destacamos uma das desvantagens da natureza determinística dos BPEs: como divide as palavras em sequências de subgleas exclusivas, apenas palavras raras são divididas em subpainhas. Isso força sequências frequentes de caracteres a aparecer principalmente em um texto segmentado como parte de tokens maiores, e não como tokens individuais. Para mostrar isso, para cada token no vocabulário do BPE, calculamos com que frequência ele aparece em um texto segmentado como um token individual e como uma sequência de caracteres (que pode figura 5: a distribuição da taxa de token para o mesmo vocabulário de textos. (A taxa de token / substring de um token é a proporção entre sua frequência como token individual e como uma sequência de caracteres.) fazer parte de um token maior ou um token individual). A Figura 5 mostra a distribuição da razão entre a frequência da substring como um token individual e como uma sequência de caracteres (para subtrings mais frequentes de 10%). Para substâncias frequentes, a distribuição da taxa de token para substring é claramente deslocada para zero, o que confirma nossa hipótese: sequências frequentes de caracteres raramente aparecem em um texto segmentado como tokens individuais. Quando um texto é segmentado usando o BPE-Dropout com o mesmo vocabulário, essa distribuição se afasta significativamente de zero, o que significa que substringas frequentes aparecem em um texto segmentado como tokens individuais com mais frequência. 6.2 Propriedades das incorporações instruídas Agora, analisaremos espaços de incorporação aprendidos por diferentes modelos. Tomamos incorporações aprendidas por modelos treinados com BPE e BPE-Dropout e para cada token olhar para os vizinhos mais próximos no espaço de incorporação correspondente. A Figura 6 mostra vários exemplos. Em contraste com o BPE, os vizinhos mais próximos de um token no espaço de incorporação do BPE-Dropout são frequentemente tokens que compartilham sequências de personagens com o token original. Para verificar essa observação quantitativamente, calculamos a precisão de 4 gramas dos 10 vizinhos dos 10 vizinhos: a proporção desses 4 gramas dos 10 principais vizinhos mais próximos que estão presentes entre os 4gramas do token original. Como esperado, as incorporações do BPE-Dropout têm maior precisão de 4gramas de caracteres (0,29) em comparação com a precisão do BPE (0,18). Isso também se refere ao estudo de Gong et al. (2018). Para várias tarefas, eles analisam o EM 1888 Figura 6: Exemplos de vizinhos mais próximos no espaço de incorporação de origem dos modelos treinados com BPE e BPE-Dropout. Modelos treinados no WMT14 EN-FR (4M). Fonte BPE BPE-DROPOUT DIFT EM ORIGINAL 27.41 28.01 +0,6 MISTELLED 24,45 26.03 +1,58 (A) BPE (B) BPE-DROPOUT Figura 7: Visualização de incorporações de fonte. Modelos treinados no WMT14 EN-FR (4M). Espaço de cama aprendido por um modelo. Os autores descobrem que, embora um token popular geralmente tenha vizinhos semanticamente relacionados, uma palavra rara geralmente não: uma grande maioria dos vizinhos mais próximos de palavras raras são palavras raras. Para confirmar isso, reduzimos a dimensionalidade das incorporações por SVD e visualizamos (Figura 7). Para o modelo treinado com BPE, os tokens raros são em geral separados do restante; Para o modelo treinado com o BPE-Dropout, esse não é o caso. Enquanto alivia essa questão Gong et al. (2018) propõem o uso de treinamento adversário para incorporar camadas, mostramos que um modelo treinado com o modelo BPE-Dropout não tem esse problema. 6.3 Robustez para modelos de entrada incorretos treinados com o BPE-Dropout melhor aprender a composicionalidade das palavras e o significado das subpainhas, o que sugere que esses modelos devem ser mais robustos ao ruído. Verificamos isso medindo a qualidade da tradução dos modelos em um conjunto de testes aumentado com erros de ortografia sintética. Aumentamos o lado da fonte de um conjunto de teste modificando cada palavra com a probabilidade de 10% aplicando uma das operações predefinidas. As operações que consideramos são (1) a remoção de um caractere de uma palavra, (2) inserção de um caractere aleatório em uma palavra, (3) substituição de um personagem em uma palavra com uma aleatória. Este aumento produz palavras de des-en 32.69 34.19 +1,5 com erros ortográficos 29.71 32.03 +2.32 EN-FR (4M) ORIGINAL 33.38 33.85 +0.47 MISPELLED 30.30 32.13 +1.83 EN-FR (16m) Original 34.37 34.822 +.5.4.4.4.4.4 32.13 +1.83 EN-FR (16m) Original 34.37 34.822. As pontuações BLEU para modelos treinados no conjunto de dados WMT14 avaliados, dada a fonte original e com ortografia. Para o EN-FR treinado em pares de frases de 16 m, o desvio BPE- foi usado apenas no lado da fonte (Seção 5.2). com a distância de edição de 1 das palavras não modificadas. A distância da edição é comumente usada para modelar erros de ortografia (Brill e Moore, 2000; Ahmad e Kondrak, 2005; Pinter et al., 2017). A Tabela 5 mostra a qualidade da tradução dos modelos treinados no conjunto de dados WMT 14 quando recebeu a fonte original e aumentada com erros de ortografia. Escolhemos deliberadamente grandes conjuntos de dados, onde as melhorias no uso do BPE-Dropout são menores. Podemos ver que, para os testes originais, as melhorias do uso do BPE-Dropout são geralmente modestas, para o conjunto de testes com ortografia, as melhorias são muito maiores: 1.6-2.3 Bleu. Isso é especialmente interessante, pois os modelos não foram expostos a erros de ortografia durante o treinamento. Portanto, mesmo para grandes conjuntos de dados usando o BPE-Dropout, pode resultar em uma qualidade substancialmente melhor para aplicações práticas, onde é provável que a entrada seja barulhenta. 1889 7 O trabalho relacionado mais próximo de nosso trabalho em motivação é o trabalho de Kudo (2018), que introduziu a estrutura de regularização da subglema, vários candidatos a segmentação e um novo algoritmo de segmentação. Outros algoritmos de segmentação incluem Creutz e Lagus (2006), Schuster e Nakajima (2012), Chitnis e Denero (2015), Kunchukuttan e Bhattacharyya (2016), Wu e Zhao (2018), Banerjee e Bhattacharya (2018). As técnicas de regularização são amplamente utilizadas para treinar redes neurais profundas. Entre as regularizações aplicadas a pesos de uma rede, os mais populares estão o abandono (Srivastava et al., 2014) e a regularização de L 2. Técnicas de aumento de dados no processamento de linguagem natural incluem soltar tokens em posições aleatórias ou trocar fichas em posições próximas (Iyyer et al., 2015; Artetxe et al., 2018; Lample et al., 2018), substituindo os tokens em posições aleatórias com um token de espaço em que algum sampo de samas (Xie e al. (por exemplo, com base na frequência do token ou em um modelo de idioma) (Fadaee et al., 2017; Xie et al., 2017; Kobayashi, 2018). Embora o BPE-Dropout possa ser considerado uma regularização, nossa motivação não é tornar um modelo robusto, injetando ruído. Ao expor um modelo a diferentes segmentações, queremos ensiná -lo a entender melhor a composição das palavras e as subpainhas e torná -la mais flexível na escolha da segmentação durante a inferência. Vários trabalhos estudam como a qualidade da tradução depende de um nível de granularidade de uma segmentação (Cherry et al., 2018; Kreutzer e Sokolov, 2018; Ding et al., 2019). Cherry et al. (2018) mostram que os modelos treinados por tempo suficiente no nível de caracteres tendem a ter melhor qualidade, mas vem com o aumento do custo computacional para treinamento e inferência. Kreutzer e Sokolov (2018) descobrem que, dada a flexibilidade na escolha do nível de segmentação, o modelo prefere operar no nível do caractere (quase). Ding et al. (2019) exploram o efeito do tamanho do vocabulário do BPE e descobrem que é melhor usar o pequeno vocabulário para configuração de baixo recurso e grande vocabulário para um ambiente de alto relevo. Seguindo essas observações, em nossos experimentos, usamos o tamanho de vocabulário diferente, dependendo do tamanho do conjunto de dados para garantir as linhas de base mais fortes. 8 Conclusões Introduzimos a regularização simples e eficaz da subglema BPE-Dropout, que opera dentro da estrutura padrão do BPE. A única diferença do BPE é como uma palavra é segmentada durante o treinamento do modelo: o BPE-Dropout solta aleatoriamente algumas mescladas da tabela de mesclagem BPE, o que resulta em diferentes segmentações para a mesma palavra. Os modelos treinados com o BPE-Dropout (1) superam o BPE e a regularização anterior da subglema em uma ampla gama de tarefas de tradução, (2) têm melhor qualidade das incorporações aprendidas, (3) são mais robustos para a entrada barulhenta. As direções futuras da pesquisa incluem taxas de abandono adaptativo para diferentes mesclares e uma análise aprofundada de outras patologias em incorporações de token aprendidas para diferentes segmentações. Agradecimentos Agradecemos aos revisores anônimos pelo feedback útil, Rico Sennrich pelos comentários valiosos sobre a primeira versão deste artigo e a equipe de tradução de máquinas Yandex para discussões e inspiração.